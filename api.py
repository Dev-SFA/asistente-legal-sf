import os
import uvicorn
import requests
import json 
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from pinecone import Pinecone
from openai import OpenAI
from fastapi.middleware.cors import CORSMiddleware
# Librer칤as necesarias para SendGrid API
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail

# --- CONFIGURACI칍N DE MODELOS Y L칈MITES ---
INDEX_NAME = "sf-abogados-01"
EMBEDDING_MODEL = "text-embedding-ada-002"
GENERATION_MODEL = "gpt-4o-mini" # Modelo Correcto
TOP_K = 5

# --- CONTACTOS Y DETALLES DE VENTA ---
PHONE_NUMBER = "+593 98 375 6678"
SALES_EMAIL = "leads@abogados-sf.com" 
CONSULTATION_COST = "40 USD"
CONSULTATION_CREDIT_MESSAGE = f"Recuerda que este monto, en caso de que llevemos contigo el caso, **se acredita al costo total del servicio como descuento**."

# --- MODELO DE DATOS DE ENTRADA (INCLUYE MEMORIA DE CHAT) ---
class QueryModel(BaseModel):
    """Define la estructura de la solicitud JSON que recibir치 el API."""
    question: str
    recaptcha_token: str
    history: list[dict] = [] # ACEPTA EL HISTORIAL

# --- INICIALIZACI칍N DE FASTAPI Y CORS ---

app = FastAPI(title="Asistente Legal SF API (RAG con GPT-4o Mini)")

# 游 CONFIGURACI칍N DE CORS
origins = ["https://abogados-sf.com", "http://localhost", "http://localhost:8000", "http://localhost:8080"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins, allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- INICIALIZACI칍N DE CLIENTES ---
# Definiciones globales iniciales
pc = None
openai_client = None
pinecone_index = None
SENDGRID_API_KEY = None # Declaraci칩n global

try:
    PORT = int(os.environ.get("PORT", 8080))
    PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    RECAPTCHA_SECRET_KEY = os.environ.get("RECAPTCHA_SECRET_KEY")
    PINECONE_ENVIRONMENT = os.environ.get("PINECONE_ENVIRONMENT")
    
    # NUEVA VARIABLE DE ENTORNO REQUERIDA para SendGrid
    SENDGRID_API_KEY = os.environ.get("SENDGRID_API_KEY") # Se obtiene la clave desde Cloud Run

    # CHEQUEO DE VARIABLES
    missing_vars = []
    if not PINECONE_API_KEY: missing_vars.append("PINECONE_API_KEY")
    if not OPENAI_API_KEY: missing_vars.append("OPENAI_API_KEY")
    if not RECAPTCHA_SECRET_KEY: missing_vars.append("RECAPTCHA_SECRET_KEY")
    if not PINECONE_ENVIRONMENT: missing_vars.append("PINECONE_ENVIRONMENT")
    if not SENDGRID_API_KEY: missing_vars.append("SENDGRID_API_KEY") # Chequeo de la nueva clave API

    if missing_vars:
        raise ValueError(f"Faltan variables de entorno esenciales: {', '.join(missing_vars)}")

    # Inicializaci칩n de clientes
    pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    pinecone_index = pc.Index(INDEX_NAME)

except Exception as e:
    print(f"ERROR FATAL DE INICIALIZACI칍N: {e}")
    raise e


# --- L칍GICA DE ENV칈O DE EMAIL (V칈A SENDGRID API) ---

def send_summary_email(subject: str, body: str, recipient: str = SALES_EMAIL):
    """
    Funci칩n para enviar el resumen interno por correo electr칩nico usando la API de SendGrid.
    """
    
    # Usa la variable global ya obtenida
    if not SENDGRID_API_KEY:
        print("ERROR DE CONFIGURACI칍N: SENDGRID_API_KEY no definida. Email no enviado.")
        return False
        
    try:
        # 1. Parsear el Subject y el Body del texto generado por el LLM
        # El LLM genera el subject y el body dentro del mismo string (summary_content)
        if "Subject:" in body:
            # Buscamos el 칤ndice del "Body:" para separar
            body_index = body.find("Body:")
            
            if body_index != -1:
                # El subject es todo lo que est치 entre "Subject:" y "Body:"
                subject_line = body.split("Subject:")[1].split("Body:")[0].strip()
                # El body es todo lo que est치 despu칠s de "Body:"
                body_content = body[body_index + len("Body:"):].strip()
            else:
                # Fallback de seguridad si el formato del LLM es incorrecto
                print("ADVERTENCIA: Formato de LLM inesperado (Body: no encontrado). Usando texto crudo.")
                subject_line = "Alerta de Lead: Revisi칩n Manual de Contenido"
                body_content = body
        else:
            # Fallback en caso de que el formato del LLM sea incorrecto (no tiene ni Subject:)
            print("ADVERTENCIA: Formato de LLM inesperado. Usando texto crudo.")
            subject_line = "Alerta de Lead: Revisi칩n Manual de Contenido"
            body_content = subject
            
    except Exception:
        # Fallback de seguridad
        subject_line = "Error Inesperado en el Lead"
        body_content = "Contenido fallido: " + subject

    try:
        # 2. Crear el objeto Mail
        message = Mail(
            from_email=SALES_EMAIL,              
            to_emails=recipient,                 
            subject=subject_line,
            plain_text_content=body_content      
        )
        
        # 3. Conectar y enviar (la API usa HTTPS/443, no el puerto SMTP)
        sg = SendGridAPIClient(SENDGRID_API_KEY)
        response = sg.send(message)

        # 4. Revisar la respuesta de la API (200 o 202 es 칠xito)
        if response.status_code in [200, 202]:
            print(f"칄XITO: Email de resumen enviado a {recipient}. C칩digo: {response.status_code}")
            return True
        else:
            print(f"ERROR SG: Fallo al enviar email. C칩digo: {response.status_code}. Cuerpo: {response.body}")
            return False

    except Exception as e:
        print(f"ERROR FATAL al enviar email por SendGrid: {e}")
        return False

# --- L칍GICA DE SEGURIDAD (reCAPTCHA) ---
async def validate_recaptcha(token: str, min_score: float = 0.5):
    if token == 'EsteEsUnTokenDePruebaTemporal':
        return True

    response = requests.post(
        'https://www.google.com/recaptcha/api/siteverify',
        data={'secret': RECAPTCHA_SECRET_KEY, 'response': token}
    )
    result = response.json()
    if result.get('success') and result.get('score', 0) >= min_score:
        return True
    else:
        return False

# --- L칍GICA RAG Y EMBEDDINGS (SIN CAMBIOS) ---
def generate_embedding(text):
    response = openai_client.embeddings.create(input=[text], model=EMBEDDING_MODEL)
    return response.data[0].embedding

def retrieve_context(embedding):
    query_results = pinecone_index.query(
        vector=embedding,
        top_k=TOP_K,
        include_metadata=True
    )
    return query_results

def generate_final_response(query, context, history):
    """
    Genera la respuesta final utilizando el contexto, la memoria (history)
    y el Super Prompt final.
    """
    # --- SUPER PROMPT COMPLETO (CON CORRECCIONES) ---
    system_prompt = (
        "Eres Agorito, un Asistente Legal Virtual, experto en Derecho Constitucional, Civil y de Familia de la ley Ecuatoriana. "
        "Tu personalidad es **vendedora, carism치tica y siempre profesional**. "
        "Tus objetivos principales son: 1) Proporcionar un an치lisis legal preliminar (Nivel 6-7), basado en RAG; y 2) Guiar clientes potenciales a la Consulta de Pago **con SF Abogados**. Prioriza SIEMPRE la conversi칩n. "

        # Principios de Operaci칩n (L칈MITES CLAVE)
        "**Filosof칤a de Operaci칩n (L칈MITES Y LIBERTAD):** "
        
        # 1. Empat칤a (El L칤mite del Tono)
        "1. **L칩gica de Empat칤a (Controlada):** Usa empat칤a solo en la primera respuesta a un problema sensible. S칠 **breve y profesional**. Luego, cambia el tono a uno directo y de an치lisis. **PROHIBIDO** el tono 'lamentero' o la compasi칩n excesiva. "
        
        # 2. Interrogaci칩n (El L칤mite de la Entrada)
        f"2. **L칩gica de Interrogaci칩n:** En la **primera interacci칩n** con el cliente (que contenga una consulta legal), y despu칠s de una breve frase de empat칤a, solicita los 5 datos clave (QU칄, QUI칄N, CU츼NDO, D칍NDE, CIUDAD/UBICACI칍N) de forma **directa y concisa**. Est치 **PROHIBIDO** repetir el saludo inicial ('춰Hola! Soy Agorito...') ya que el frontend lo maneja. "
        
        # 3. Contraste (El L칤mite de la Especialidad)
        "3. **L칩gica de Contraste (Especialidad):** Lim칤tate ESTRICTAMENTE a Derecho Constitucional, Civil y de Familia. Si el tema es de otra rama o no est치 en RAG, aplica la **Regla de Cierre de Contraste** inmediatamente: 'Lamentablemente, ese asunto est치 fuera de nuestra especialidad. Si lo desea, puede contactarnos directamente al {PHONE_NUMBER} para ver si podemos recomendarle un colega.' (Una vez en fase de venta (CTA), ignora los bajos resultados RAG). "
        
        # 5. Cierre y Nutrici칩n (El L칤mite de la Venta - CORRECCI칍N CLAVE AQU칈)
        "5. **L칩gica de Cierre y Nutrici칩n:** Despu칠s de dar el an치lisis preliminar (Nivel 6-7), **DEBES** hacer un Call-to-Action (CTA) expl칤cito. **PROHIBIDO usar frases gen칠ricas** como 'buscar asesor칤a legal'. Dirige SIEMPRE a la firma. "
        " - **Formato del CTA 칔nico (Gu칤a, NO Script):** Utiliza un formato similar a: 'Te recomendar칤a [acci칩n espec칤fica] y que consideres buscar asesor칤a legal **con nuestro equipo**. 쮻eseas agendar tu **Consulta de Pago de {CONSULTATION_COST}** (acreditable, {CONSULTATION_CREDIT_MESSAGE})? 쯊e gustar칤a que te env칤e los pasos para agendar la consulta?'"
        " - **Flujo de Datos (MEMORIA ESTRICTA Y ACUMULATIVA - REFORZADA):** Si el cliente acepta el CTA, **DEBES** solicitar los **4 DATOS CLAVE**: 1. Nombre completo, 2. WhatsApp, 3. Correo, **4. Preferencia de Consulta (Presencial/Virtual)**. **PROHIBIDO** solicitar fecha/hora o direcci칩n exacta. **MEMORIA ESTRICTA Y ACUMULATIVA REFORZADA**: Debes reconocer y acumular **todos** los datos que el cliente te proporcione en cualquier mensaje, **INCLUYENDO CUALQUIER DATO PROVISTO EN MENSAJES ANTERIORES DONDE EL ASISTENTE FALL칍**. **NUNCA DEBES REPETIR** la lista de 4 puntos. Solo pregunta de forma cort칠s por **el/los dato(s) EXACTO(S) que FALTA(N)**. Una vez que se tienen los 4 datos: 1) Genera el Resumen Interno (ENVUELTO en [INTERNAL_SUMMARY_START]...[INTERNAL_SUMMARY_END]) y 2) **ENV칈A 칔NICAMENTE** el mensaje final de confirmaci칩n: **'춰Perfecto! Ya tengo toda la informaci칩n. Pronto alguien de nuestro equipo se pondr치 en contacto contigo a trav칠s de tu [WhatsApp o correo] para coordinar la fecha y hora de tu consulta de {CONSULTATION_COST}, que se acreditar치 al costo total del servicio.'** "

        # Reglas de Conversaci칩n (LIBERTAD Y GU칈A)
        "**Reglas de Conversaci칩n:** "
        " - **Tono:** Profesional, carism치tico y orientado a la soluci칩n. Utiliza negritas, listas y subt칤tulos (##) de forma natural para organizar el an치lisis (LIBERTAD en el formato, pero USA Markdown). "
        " - **Nivel de Informaci칩n:** Nivel 6 a 7 (detallado y 칰til). **PROHIBIDO** citar art칤culos o dar pasos a seguir (para obligar la consulta). "
        " - **PROHIBICI칍N CLAVE:** NO alucinar o inventar datos. S칠 honesto si el contexto RAG es d칠bil. "
        f" - **Meta de Venta:** El objetivo es la consulta de {CONSULTATION_COST} (acreditable). "
        f" - **Cese de Interacci칩n:** **CESA INMEDIATAMENTE TODA INTERACCI칍N** despu칠s de enviar el mensaje final de confirmaci칩n de datos."
        f" - **Transferencia a Humano (BLINDADA):** Si el cliente se frustra por la respuesta o el caso es objetivamente complejo o el LLM no tiene contexto RAG, aplica: 'Entiendo su preocupaci칩n. Este caso requiere la atenci칩n de uno de nuestros abogados. Por favor, cont치ctenos directamente al {PHONE_NUMBER} o env칤e un correo a {SALES_EMAIL}.' **ESTA REGLA EST츼 PROHIBIDA EN SU TOTALIDAD SI EL CLIENTE YA HA DICHO 'S칈' A LA CONSULTA O EST츼 EN PROCESO DE ENTREGA DE DATOS.**"

        # Formato del Resumen (Uso Interno)
        "**Condiciones de Resumen (Generar para {SALES_EMAIL}):** Genera un resumen cuando el cliente ha provisto sus 4 datos. "
        "**Formato del Resumen (Uso Interno de la IA):** Subject: [New Prospect - Legal Advice] o [High-Value Prospect]. Body: **Client Details:** Name: [Name], WhatsApp Number: [Number], Email: [Email, if available], **Consultation Type:** [Presencial/Virtual], City/Location: [Client's City/Location]. **Case Analysis (For Internal Use):** Legal Branch: [Relevant branch of law], Problem Summary: [Brief description of the legal problem.], Key Points: [Identify crucial facts and documents that are needed.]. **Recommendation to the Firm:** [Suggest 1 o 2 pasos inmediatos]. **Client's Objective:** [Describe lo que el cliente desea lograr]."
    )

    # 3. Formatear el Contexto RAG y la Pregunta
    context_text = "\n\n".join([item['metadata']['text'] for item in context.matches])

    rag_prompt = (
        f"CONTEXTO PROPORCIONADO PARA EL AN츼LISIS (RAG):\n{context_text}\n\n"
        f"Pregunta m치s reciente del Usuario: {query}"
    )

    # 4. Construir la Matriz de Mensajes (Super Prompt + Memoria + Pregunta)
    messages = [
        {"role": "system", "content": system_prompt}
    ]

    # A침adir historial de conversaci칩n
    messages.extend(history)

    # A침adir el prompt RAG (Contexto + Pregunta actual)
    messages.append({"role": "user", "content": rag_prompt})

    response = openai_client.chat.completions.create(
        model=GENERATION_MODEL,
        messages=messages,
        temperature=0.0 
    )

    final_response_text = response.choices[0].message.content

    return final_response_text

# --- ENDPOINT PRINCIPAL (SIN CAMBIOS) ---

@app.post("/query")
async def process_query(data: QueryModel):
    """Endpoint principal para recibir la pregunta y devolver la respuesta."""
    try:
        if not await validate_recaptcha(data.recaptcha_token):
              raise HTTPException(status_code=403, detail="Validaci칩n reCAPTCHA fallida. Acceso denegado.")

        query_embedding = generate_embedding(data.question)
        query_results = retrieve_context(query_embedding)

        # 1. Generar la respuesta (que incluye el resumen interno y el mensaje al usuario)
        raw_llm_response = generate_final_response(data.question, query_results, data.history)

        # 2. L칩gica para DETECTAR y ENVIAR el resumen
        summary_start_tag = "[INTERNAL_SUMMARY_START]"
        summary_end_tag = "[INTERNAL_SUMMARY_END]"
        
        # Verificar si hay un resumen interno para enviar
        if summary_start_tag in raw_llm_response and summary_end_tag in raw_llm_response:
            try:
                # Extraer el contenido del resumen
                summary_content = raw_llm_response.split(summary_start_tag)[1].split(summary_end_tag)[0].strip()
                
                # Intentar enviar el correo (USA LA NUEVA FUNCI칍N DE SENDGRID)
                send_summary_email(summary_content, summary_content)
                
                # 3. Limpiar la respuesta para el usuario (eliminar el resumen y las etiquetas)
                user_response = raw_llm_response.replace(summary_start_tag + summary_content + summary_end_tag, "").strip()
            except Exception as e:
                # Si falla el parseo o el env칤o, se registra el error y se env칤a la respuesta cruda (o se limpia solo las etiquetas)
                print(f"Advertencia: Fallo en el procesamiento del resumen interno. {e}")
                user_response = raw_llm_response.replace(summary_start_tag, "").replace(summary_end_tag, "").strip()
        else:
            # Si no hay etiquetas, la respuesta va directamente al usuario
            user_response = raw_llm_response

        return {"answer": user_response}

    except Exception as e:
        print(f"Error procesando la consulta: {e}")
        raise HTTPException(status_code=500, detail="Error interno del servidor al procesar la solicitud.")

# --- INICIO LOCAL (Para pruebas) ---
if __file__ == "__main__":
    port_local = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port_local)
